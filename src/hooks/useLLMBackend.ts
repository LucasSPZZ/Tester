import { useState } from 'react';
import { getCurrentBackendUrl } from '../config/backend';
import type { SystemPrompt, Message } from '../types/prompt';

const BACKEND_URL = getCurrentBackendUrl(); // URL do backend sempre atualizada

interface LLMOptions {
  temperature?: number;
  maxTokens?: number;
  model?: string;
}

interface LLMResponse {
  success: boolean;
  sql?: string;
  response?: string;
  error?: string;
  tokensUsed?: any;
}

interface ConversationContext {
  systemPrompt: SystemPrompt;
  messageHistory: Message[];
  newUserMessage: string;
  conversationId: string;
  agentPromptId: string;
  model?: string; // âœ¨ NOVO: Modelo OpenRouter configurado
}

export const useLLMBackend = () => {
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState<string | null>(null);

  // ğŸš€ NOVA FUNÃ‡ÃƒO PRINCIPAL: Processar mensagem com contexto completo
  const sendMessageWithContext = async (context: ConversationContext): Promise<string> => {
    setIsProcessing(true);
    setError(null);

    try {
      console.log('ğŸ¤– [LLM] Enviando mensagem com contexto completo...');
      console.log('ğŸ“‹ Contexto:', {
        agentName: context.systemPrompt.name,
        historyLength: context.messageHistory.length,
        newMessage: context.newUserMessage.substring(0, 100) + '...'
      });

      // Detectar tipo de prompt para escolher endpoint correto
      const promptType = detectPromptType(context.systemPrompt);
      
      let response: string;
      
      if (promptType === 'sql') {
        // Usar endpoint SQL especializado
        response = await processSQLConversation(context);
      } else {
        // Usar endpoint de chat geral
        response = await processGeneralConversation(context);
      }

      console.log('âœ… [LLM] Resposta recebida:', response.substring(0, 100) + '...');
      return response;

    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Erro desconhecido';
      console.error('âŒ [LLM] Erro ao processar:', errorMessage);
      setError(errorMessage);
      
      // Fallback inteligente com contexto
      return generateContextualFallback(context, errorMessage);
    } finally {
      setIsProcessing(false);
    }
  };

  // ğŸ¯ Processar conversa SQL com contexto
  const processSQLConversation = async (context: ConversationContext): Promise<string> => {
    console.log('ğŸ—„ï¸ [SQL] Processando conversa SQL...');
    
    // Formatar mensagens no formato especÃ­fico solicitado
    const formattedMessages = [`System: ${context.systemPrompt.content}`];
    
    // Adicionar histÃ³rico da conversa
    context.messageHistory.forEach(msg => {
      const role = msg.role === 'user' ? 'Human' : 'AI';
      formattedMessages.push(`${role}: ${msg.content}`);
    });
    
    // Adicionar nova mensagem do usuÃ¡rio
    formattedMessages.push(`Human: ${context.newUserMessage}`);
    
    const payload = {
      messages: formattedMessages,
      estimatedTokens: formattedMessages.join(' ').length / 4, // Estimativa simples
      conversation_context: {
        agent_id: context.agentPromptId,
        conversation_id: context.conversationId,
        agent_name: context.systemPrompt.name
      },
      prompt_type: 'sql',
      options: {
        model: context.model || "anthropic/claude-3.5-sonnet", // âœ¨ NOVO: Usar modelo OpenRouter configurado
        temperature: 0.1,
        timeout: 60000,
        max_retries: 2
      }
    };

    console.log('ğŸ“¤ [SQL] Enviando payload formatado:', {
      messagesCount: formattedMessages.length,
      estimatedTokens: payload.estimatedTokens,
      model: payload.options.model
    });
    
    console.log('ğŸ“‹ [SQL] Mensagens formatadas:');
    formattedMessages.forEach((msg, index) => {
      console.log(`  ${index + 1}. ${msg.substring(0, 100)}${msg.length > 100 ? '...' : ''}`);
    });
    
    console.log('ğŸ“¦ [SQL] Payload completo:', JSON.stringify(payload, null, 2));

    const response = await fetch(`${BACKEND_URL}/api/chat-sql`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      throw new Error(`Erro HTTP: ${response.status} - ${response.statusText}`);
    }

    const result: LLMResponse = await response.json();
    
    if (!result.success) {
      throw new Error(result.error || 'Erro na resposta da LLM');
    }

    return result.response || result.sql || 'Resposta vazia da LLM';
  };

  // ğŸ’¬ Processar conversa geral com contexto
  const processGeneralConversation = async (context: ConversationContext): Promise<string> => {
    console.log('ğŸ’¬ [CHAT] Processando conversa geral...');
    
    // Formatar mensagens no formato especÃ­fico solicitado
    const formattedMessages = [`System: ${context.systemPrompt.content}`];
    
    // Adicionar histÃ³rico da conversa
    context.messageHistory.forEach(msg => {
      const role = msg.role === 'user' ? 'Human' : 'AI';
      formattedMessages.push(`${role}: ${msg.content}`);
    });
    
    // Adicionar nova mensagem do usuÃ¡rio
    formattedMessages.push(`Human: ${context.newUserMessage}`);
    
    const payload = {
      messages: formattedMessages,
      estimatedTokens: formattedMessages.join(' ').length / 4, // Estimativa simples
      conversation_context: {
        agent_id: context.agentPromptId,
        conversation_id: context.conversationId,
        agent_name: context.systemPrompt.name,
        agent_description: context.systemPrompt.description
      },
      prompt_type: 'general',
      options: {
        model: context.model || "anthropic/claude-3.5-sonnet", // âœ¨ NOVO: Usar modelo OpenRouter configurado
        temperature: 0.7,
        timeout: 60000,
        max_retries: 2
      }
    };

    console.log('ğŸ“¤ [CHAT] Enviando payload formatado:', {
      messagesCount: formattedMessages.length,
      estimatedTokens: payload.estimatedTokens,
      model: payload.options.model
    });
    
    console.log('ğŸ“‹ [CHAT] Mensagens formatadas:');
    formattedMessages.forEach((msg, index) => {
      console.log(`  ${index + 1}. ${msg.substring(0, 100)}${msg.length > 100 ? '...' : ''}`);
    });
    
    console.log('ğŸ“¦ [CHAT] Payload completo:', JSON.stringify(payload, null, 2));

    const response = await fetch(`${BACKEND_URL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      throw new Error(`Erro HTTP: ${response.status} - ${response.statusText}`);
    }

    const result: LLMResponse = await response.json();
    
    if (!result.success) {
      throw new Error(result.error || 'Erro na resposta da LLM');
    }

    return result.response || 'Resposta vazia da LLM';
  };

  // ğŸ” Detectar tipo de prompt
  const detectPromptType = (systemPrompt: SystemPrompt): 'sql' | 'general' => {
    const content = systemPrompt.content.toLowerCase();
    const name = systemPrompt.name.toLowerCase();
    const description = (systemPrompt.description || '').toLowerCase();
    
    const sqlKeywords = ['sql', 'postgresql', 'database', 'supabase', 'query', 'funÃ§Ã£o', 'function', 'trigger', 'rpc'];
    
    const isSQL = sqlKeywords.some(keyword => 
      content.includes(keyword) || name.includes(keyword) || description.includes(keyword)
    );
    
    return isSQL ? 'sql' : 'general';
  };

  // ğŸ›¡ï¸ Fallback contextual inteligente
  const generateContextualFallback = (context: ConversationContext, errorMessage: string): string => {
    const promptType = detectPromptType(context.systemPrompt);
    
    const fallbackResponse = `ğŸ¤– **${context.systemPrompt.name}** (Modo Simulado)\n\n` +
      `âŒ **Erro de conexÃ£o:** ${errorMessage}\n\n` +
      `ğŸ“ **Sua mensagem:** "${context.newUserMessage}"\n\n` +
      `ğŸ“Š **Contexto da conversa:** ${context.messageHistory.length} mensagens anteriores\n\n`;

    if (promptType === 'sql') {
      return fallbackResponse + 
        `ğŸ—„ï¸ **Resposta SQL Simulada:**\n\n` +
        `\`\`\`sql\n-- Baseado na sua solicitaÃ§Ã£o e histÃ³rico da conversa\n-- Esta seria uma funÃ§Ã£o/query personalizada para:\n-- "${context.newUserMessage}"\n\nSELECT 'Conecte ao backend para resposta real da IA' as message;\n\`\`\`\n\n` +
        `ğŸ’¡ **Para ativar IA real:**\n1. Inicie o backend: \`npm run dev\` na pasta backend\n2. Configure GEMINI_API_KEY\n3. A prÃ³xima mensagem serÃ¡ processada pela IA`;
    } else {
      return fallbackResponse +
        `ğŸ’¬ **Resposta Simulada:**\n\n` +
        `Com base no seu prompt "${context.systemPrompt.name}" e no histÃ³rico de ${context.messageHistory.length} mensagens, ` +
        `eu processaria sua solicitaÃ§Ã£o de forma personalizada.\n\n` +
        `ğŸ”— **Conecte ao backend** para respostas reais da IA com contexto completo.`;
    }
  };

  // ğŸ”„ FUNÃ‡ÃƒO LEGACY: Manter compatibilidade (DEPRECATED)
  const processMessage = async (
    userMessage: string,
    systemPromptContent: string,
    geminiApiKey?: string
  ): Promise<string> => {
    console.warn('âš ï¸ [DEPRECATED] Use sendMessageWithContext() para contexto completo');
    
    // Simular contexto bÃ¡sico para compatibilidade
    const basicContext: ConversationContext = {
      systemPrompt: {
        id: 'legacy',
        name: 'Legacy Prompt',
        content: systemPromptContent,
        createdAt: new Date().toISOString(),
        updatedAt: new Date().toISOString()
      },
      messageHistory: [],
      newUserMessage: userMessage,
      conversationId: 'legacy',
      agentPromptId: 'legacy'
    };

    return await sendMessageWithContext(basicContext);
  };

  // ğŸ—„ï¸ FunÃ§Ã£o para gerar SQL via backend (LEGACY)
  const generateSQL = async (
    userPrompt: string,
    systemPromptContent: string,
    geminiApiKey?: string
  ): Promise<LLMResponse> => {
    const response = await fetch(`${BACKEND_URL}/api/generate-sql`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        userPrompt,
        schema: {
          tables: [],
          functions: []
        },
        geminiApiKey
      }),
    });

    if (!response.ok) {
      throw new Error(`Erro HTTP: ${response.status}`);
    }

    return await response.json();
  };

  // ğŸ”§ Testar conexÃ£o com backend
  const testBackendConnection = async (): Promise<boolean> => {
    try {
      const response = await fetch(`${BACKEND_URL}/api/health`);
      return response.ok;
    } catch (error) {
      return false;
    }
  };

  // ğŸ”‘ Testar API do Gemini
  const testGeminiAPI = async (apiKey?: string): Promise<boolean> => {
    try {
      const params = apiKey ? `?apiKey=${encodeURIComponent(apiKey)}` : '';
      const response = await fetch(`${BACKEND_URL}/api/test-gemini${params}`);
      const result = await response.json();
      return result.success;
    } catch (error) {
      return false;
    }
  };

  return {
    // ğŸš€ NOVA API PRINCIPAL
    sendMessageWithContext,
    
    // ğŸ”„ LEGACY SUPPORT
    processMessage,
    generateSQL,
    
    // ğŸ”§ UTILITIES
    testBackendConnection,
    testGeminiAPI,
    
    // ğŸ“Š STATE
    isProcessing,
    error,
    backendUrl: BACKEND_URL,
  };
}; 